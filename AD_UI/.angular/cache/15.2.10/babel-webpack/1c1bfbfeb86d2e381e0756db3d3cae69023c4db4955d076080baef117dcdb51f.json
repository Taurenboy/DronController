{"ast":null,"code":"var _asyncToGenerator = require(\"C:/Users/ingev/Documents/Desarrollo/Luna/DronController/AD_UI/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\nconst Long = require('../../utils/long');\nconst Decoder = require('../decoder');\nconst MessageDecoder = require('../message/decoder');\nconst {\n  lookupCodecByAttributes\n} = require('../message/compression');\nconst {\n  KafkaJSPartialMessageError\n} = require('../../errors');\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator(function* (primaryDecoder, size = null) {\n    const messages = [];\n    const messageSetSize = size || primaryDecoder.readInt32();\n    const messageSetDecoder = primaryDecoder.slice(messageSetSize);\n    while (messageSetDecoder.offset < messageSetSize) {\n      try {\n        const message = EntryDecoder(messageSetDecoder);\n        const codec = lookupCodecByAttributes(message.attributes);\n        if (codec) {\n          const buffer = yield codec.decompress(message.value);\n          messages.push(...EntriesDecoder(new Decoder(buffer), message));\n        } else {\n          messages.push(message);\n        }\n      } catch (e) {\n        if (e.name === 'KafkaJSPartialMessageError') {\n          // We tried to decode a partial message, it means that minBytes\n          // is probably too low\n          break;\n        }\n        if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n          // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n          // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n          // receive the full record batch on the next request\n          break;\n        }\n        throw e;\n      }\n    }\n    primaryDecoder.forward(messageSetSize);\n    return messages;\n  });\n  return function (_x) {\n    return _ref.apply(this, arguments);\n  };\n}();\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = [];\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder));\n  }\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset);\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset);\n    const baseOffset = compressedOffset - lastMessageOffset;\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset).add(baseOffset).toString();\n    }\n  }\n  return messages;\n};\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the offset`);\n  }\n  const offset = decoder.readInt64().toString();\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the message size`);\n  }\n  const size = decoder.readInt32();\n  return MessageDecoder(offset, size, decoder);\n};","map":{"version":3,"names":["Long","require","Decoder","MessageDecoder","lookupCodecByAttributes","KafkaJSPartialMessageError","module","exports","primaryDecoder","size","messages","messageSetSize","readInt32","messageSetDecoder","slice","offset","message","EntryDecoder","codec","attributes","buffer","decompress","value","push","EntriesDecoder","e","name","forward","decoder","compressedMessage","length","magicByte","compressedOffset","fromValue","lastMessageOffset","baseOffset","add","toString","canReadInt64","readInt64","canReadInt32"],"sources":["C:/Users/ingev/Documents/Desarrollo/Luna/DronController/AD_UI/node_modules/kafkajs/src/protocol/messageSet/decoder.js"],"sourcesContent":["const Long = require('../../utils/long')\nconst Decoder = require('../decoder')\nconst MessageDecoder = require('../message/decoder')\nconst { lookupCodecByAttributes } = require('../message/compression')\nconst { KafkaJSPartialMessageError } = require('../../errors')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = []\n  const messageSetSize = size || primaryDecoder.readInt32()\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize)\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder)\n      const codec = lookupCodecByAttributes(message.attributes)\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value)\n        messages.push(...EntriesDecoder(new Decoder(buffer), message))\n      } else {\n        messages.push(message)\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break\n      }\n\n      throw e\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize)\n  return messages\n}\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = []\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder))\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset)\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset)\n    const baseOffset = compressedOffset - lastMessageOffset\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset)\n        .add(baseOffset)\n        .toString()\n    }\n  }\n\n  return messages\n}\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the offset`\n    )\n  }\n\n  const offset = decoder.readInt64().toString()\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the message size`\n    )\n  }\n\n  const size = decoder.readInt32()\n  return MessageDecoder(offset, size, decoder)\n}\n"],"mappings":";AAAA,MAAMA,IAAI,GAAGC,OAAO,CAAC,kBAAkB,CAAC;AACxC,MAAMC,OAAO,GAAGD,OAAO,CAAC,YAAY,CAAC;AACrC,MAAME,cAAc,GAAGF,OAAO,CAAC,oBAAoB,CAAC;AACpD,MAAM;EAAEG;AAAwB,CAAC,GAAGH,OAAO,CAAC,wBAAwB,CAAC;AACrE,MAAM;EAAEI;AAA2B,CAAC,GAAGJ,OAAO,CAAC,cAAc,CAAC;;AAE9D;AACA;AACA;AACA;AACA;AACA;;AAEAK,MAAM,CAACC,OAAO;EAAA,6BAAG,WAAOC,cAAc,EAAEC,IAAI,GAAG,IAAI,EAAK;IACtD,MAAMC,QAAQ,GAAG,EAAE;IACnB,MAAMC,cAAc,GAAGF,IAAI,IAAID,cAAc,CAACI,SAAS,EAAE;IACzD,MAAMC,iBAAiB,GAAGL,cAAc,CAACM,KAAK,CAACH,cAAc,CAAC;IAE9D,OAAOE,iBAAiB,CAACE,MAAM,GAAGJ,cAAc,EAAE;MAChD,IAAI;QACF,MAAMK,OAAO,GAAGC,YAAY,CAACJ,iBAAiB,CAAC;QAC/C,MAAMK,KAAK,GAAGd,uBAAuB,CAACY,OAAO,CAACG,UAAU,CAAC;QAEzD,IAAID,KAAK,EAAE;UACT,MAAME,MAAM,SAASF,KAAK,CAACG,UAAU,CAACL,OAAO,CAACM,KAAK,CAAC;UACpDZ,QAAQ,CAACa,IAAI,CAAC,GAAGC,cAAc,CAAC,IAAItB,OAAO,CAACkB,MAAM,CAAC,EAAEJ,OAAO,CAAC,CAAC;QAChE,CAAC,MAAM;UACLN,QAAQ,CAACa,IAAI,CAACP,OAAO,CAAC;QACxB;MACF,CAAC,CAAC,OAAOS,CAAC,EAAE;QACV,IAAIA,CAAC,CAACC,IAAI,KAAK,4BAA4B,EAAE;UAC3C;UACA;UACA;QACF;QAEA,IAAID,CAAC,CAACC,IAAI,KAAK,yCAAyC,EAAE;UACxD;UACA;UACA;UACA;QACF;QAEA,MAAMD,CAAC;MACT;IACF;IAEAjB,cAAc,CAACmB,OAAO,CAAChB,cAAc,CAAC;IACtC,OAAOD,QAAQ;EACjB,CAAC;EAAA;IAAA;EAAA;AAAA;AAED,MAAMc,cAAc,GAAG,CAACI,OAAO,EAAEC,iBAAiB,KAAK;EACrD,MAAMnB,QAAQ,GAAG,EAAE;EAEnB,OAAOkB,OAAO,CAACb,MAAM,GAAGa,OAAO,CAACR,MAAM,CAACU,MAAM,EAAE;IAC7CpB,QAAQ,CAACa,IAAI,CAACN,YAAY,CAACW,OAAO,CAAC,CAAC;EACtC;EAEA,IAAIC,iBAAiB,CAACE,SAAS,GAAG,CAAC,IAAIF,iBAAiB,CAACd,MAAM,IAAI,CAAC,EAAE;IACpE,MAAMiB,gBAAgB,GAAGhC,IAAI,CAACiC,SAAS,CAACJ,iBAAiB,CAACd,MAAM,CAAC;IACjE,MAAMmB,iBAAiB,GAAGlC,IAAI,CAACiC,SAAS,CAACvB,QAAQ,CAACA,QAAQ,CAACoB,MAAM,GAAG,CAAC,CAAC,CAACf,MAAM,CAAC;IAC9E,MAAMoB,UAAU,GAAGH,gBAAgB,GAAGE,iBAAiB;IAEvD,KAAK,MAAMlB,OAAO,IAAIN,QAAQ,EAAE;MAC9BM,OAAO,CAACD,MAAM,GAAGf,IAAI,CAACiC,SAAS,CAACjB,OAAO,CAACD,MAAM,CAAC,CAC5CqB,GAAG,CAACD,UAAU,CAAC,CACfE,QAAQ,EAAE;IACf;EACF;EAEA,OAAO3B,QAAQ;AACjB,CAAC;AAED,MAAMO,YAAY,GAAGW,OAAO,IAAI;EAC9B,IAAI,CAACA,OAAO,CAACU,YAAY,EAAE,EAAE;IAC3B,MAAM,IAAIjC,0BAA0B,CACjC,gFAA+E,CACjF;EACH;EAEA,MAAMU,MAAM,GAAGa,OAAO,CAACW,SAAS,EAAE,CAACF,QAAQ,EAAE;EAE7C,IAAI,CAACT,OAAO,CAACY,YAAY,EAAE,EAAE;IAC3B,MAAM,IAAInC,0BAA0B,CACjC,sFAAqF,CACvF;EACH;EAEA,MAAMI,IAAI,GAAGmB,OAAO,CAAChB,SAAS,EAAE;EAChC,OAAOT,cAAc,CAACY,MAAM,EAAEN,IAAI,EAAEmB,OAAO,CAAC;AAC9C,CAAC"},"metadata":{},"sourceType":"script","externalDependencies":[]}