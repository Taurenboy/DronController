{"ast":null,"code":"var _asyncToGenerator = require(\"C:/Users/ingev/Documents/Desarrollo/Luna/DronController/AD_UI/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\nconst Long = require('../utils/long');\nconst createRetry = require('../retry');\nconst {\n  initialRetryTime\n} = require('../retry/defaults');\nconst ConsumerGroup = require('./consumerGroup');\nconst Runner = require('./runner');\nconst {\n  events,\n  wrap: wrapEvent,\n  unwrap: unwrapEvent\n} = require('./instrumentationEvents');\nconst InstrumentationEventEmitter = require('../instrumentation/emitter');\nconst {\n  KafkaJSNonRetriableError\n} = require('../errors');\nconst {\n  roundRobin\n} = require('./assigners');\nconst {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET\n} = require('../constants');\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel');\nconst sharedPromiseTo = require('../utils/sharedPromiseTo');\nconst {\n  keys,\n  values\n} = Object;\nconst {\n  CONNECT,\n  DISCONNECT,\n  STOP,\n  CRASH\n} = events;\nconst eventNames = values(events);\nconst eventKeys = keys(events).map(key => `consumer.events.${key}`).join(', ');\nconst specialOffsets = [Long.fromValue(EARLIEST_OFFSET).toString(), Long.fromValue(LATEST_OFFSET).toString()];\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576,\n  // 1MB\n  minBytes = 1,\n  maxBytes = 10485760,\n  // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.');\n  }\n  const logger = rootLogger.namespace('Consumer');\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter();\n  const assigners = partitionAssigners.map(createAssigner => createAssigner({\n    groupId,\n    logger,\n    cluster\n  }));\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {};\n  let runner = null;\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null;\n  let restartTimeout = null;\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(`Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`);\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = /*#__PURE__*/function () {\n    var _ref = _asyncToGenerator(function* () {\n      yield cluster.connect();\n      instrumentationEmitter.emit(CONNECT);\n    });\n    return function connect() {\n      return _ref.apply(this, arguments);\n    };\n  }();\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = /*#__PURE__*/function () {\n    var _ref2 = _asyncToGenerator(function* () {\n      try {\n        yield stop();\n        logger.debug('consumer has stopped, disconnecting', {\n          groupId\n        });\n        yield cluster.disconnect();\n        instrumentationEmitter.emit(DISCONNECT);\n      } catch (e) {\n        logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n          stack: e.stack,\n          groupId\n        });\n        throw e;\n      }\n    });\n    return function disconnect() {\n      return _ref2.apply(this, arguments);\n    };\n  }();\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo( /*#__PURE__*/_asyncToGenerator(function* () {\n    try {\n      if (runner) {\n        yield runner.stop();\n        runner = null;\n        consumerGroup = null;\n        instrumentationEmitter.emit(STOP);\n      }\n      clearTimeout(restartTimeout);\n      logger.info('Stopped', {\n        groupId\n      });\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId\n      });\n      throw e;\n    }\n  }));\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = /*#__PURE__*/function () {\n    var _ref4 = _asyncToGenerator(function* ({\n      topic,\n      topics: subscriptionTopics,\n      fromBeginning = false\n    }) {\n      if (consumerGroup) {\n        throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running');\n      }\n      if (!topic && !subscriptionTopics) {\n        throw new KafkaJSNonRetriableError('Missing required argument \"topics\"');\n      }\n      if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n        throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array');\n      }\n      const subscriptions = subscriptionTopics || [topic];\n      for (const subscription of subscriptions) {\n        if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`);\n        }\n      }\n      const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp);\n      const metadata = hasRegexSubscriptions ? yield cluster.metadata() : undefined;\n      const topicsToSubscribe = [];\n      for (const subscription of subscriptions) {\n        const isRegExp = subscription instanceof RegExp;\n        if (isRegExp) {\n          const topicRegExp = subscription;\n          const matchedTopics = metadata.topicMetadata.map(({\n            topic: topicName\n          }) => topicName).filter(topicName => topicRegExp.test(topicName));\n          logger.debug('Subscription based on RegExp', {\n            groupId,\n            topicRegExp: topicRegExp.toString(),\n            matchedTopics\n          });\n          topicsToSubscribe.push(...matchedTopics);\n        } else {\n          topicsToSubscribe.push(subscription);\n        }\n      }\n      for (const t of topicsToSubscribe) {\n        topics[t] = {\n          fromBeginning\n        };\n      }\n      yield cluster.addMultipleTargetTopics(topicsToSubscribe);\n    });\n    return function subscribe(_x) {\n      return _ref4.apply(this, arguments);\n    };\n  }();\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = /*#__PURE__*/function () {\n    var _ref5 = _asyncToGenerator(function* ({\n      autoCommit = true,\n      autoCommitInterval = null,\n      autoCommitThreshold = null,\n      eachBatchAutoResolve = true,\n      partitionsConsumedConcurrently: concurrency = 1,\n      eachBatch = null,\n      eachMessage = null\n    } = {}) {\n      if (consumerGroup) {\n        logger.warn('consumer#run was called, but the consumer is already running', {\n          groupId\n        });\n        return;\n      }\n      const start = /*#__PURE__*/function () {\n        var _ref6 = _asyncToGenerator(function* (onCrash) {\n          logger.info('Starting', {\n            groupId\n          });\n          consumerGroup = new ConsumerGroup({\n            logger: rootLogger,\n            topics: keys(topics),\n            topicConfigurations: topics,\n            retry,\n            cluster,\n            groupId,\n            assigners,\n            sessionTimeout,\n            rebalanceTimeout,\n            maxBytesPerPartition,\n            minBytes,\n            maxBytes,\n            maxWaitTimeInMs,\n            instrumentationEmitter,\n            isolationLevel,\n            rackId,\n            metadataMaxAge,\n            autoCommit,\n            autoCommitInterval,\n            autoCommitThreshold\n          });\n          runner = new Runner({\n            logger: rootLogger,\n            consumerGroup,\n            instrumentationEmitter,\n            heartbeatInterval,\n            retry,\n            autoCommit,\n            eachBatchAutoResolve,\n            eachBatch,\n            eachMessage,\n            onCrash,\n            concurrency\n          });\n          yield runner.start();\n        });\n        return function start(_x2) {\n          return _ref6.apply(this, arguments);\n        };\n      }();\n      const onCrash = /*#__PURE__*/function () {\n        var _ref7 = _asyncToGenerator(function* (e) {\n          logger.error(`Crash: ${e.name}: ${e.message}`, {\n            groupId,\n            retryCount: e.retryCount,\n            stack: e.stack\n          });\n          if (e.name === 'KafkaJSConnectionClosedError') {\n            cluster.removeBroker({\n              host: e.host,\n              port: e.port\n            });\n          }\n          yield disconnect();\n          const getOriginalCause = error => {\n            if (error.cause) {\n              return getOriginalCause(error.cause);\n            }\n            return error;\n          };\n          const isErrorRetriable = e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true;\n          const shouldRestart = isErrorRetriable && (!retry || !retry.restartOnFailure || (yield retry.restartOnFailure(e).catch(error => {\n            logger.error('Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.', {\n              error: error.message || error,\n              cause: e.message || e,\n              groupId\n            });\n            return true;\n          })));\n          instrumentationEmitter.emit(CRASH, {\n            error: e,\n            groupId,\n            restart: shouldRestart\n          });\n          if (shouldRestart) {\n            const retryTime = e.retryTime || retry && retry.initialRetryTime || initialRetryTime;\n            logger.error(`Restarting the consumer in ${retryTime}ms`, {\n              retryCount: e.retryCount,\n              retryTime,\n              groupId\n            });\n            restartTimeout = setTimeout(() => start(onCrash), retryTime);\n          }\n        });\n        return function onCrash(_x3) {\n          return _ref7.apply(this, arguments);\n        };\n      }();\n      yield start(onCrash);\n    });\n    return function run() {\n      return _ref5.apply(this, arguments);\n    };\n  }();\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`);\n    }\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type);\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack\n        });\n      });\n    });\n  };\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = /*#__PURE__*/function () {\n    var _ref8 = _asyncToGenerator(function* (topicPartitions = []) {\n      const commitsByTopic = topicPartitions.reduce((payload, {\n        topic,\n        partition,\n        offset,\n        metadata = null\n      }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);\n        }\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(`Invalid partition, expected a number received ${partition}`);\n        }\n        let commitOffset;\n        try {\n          commitOffset = Long.fromValue(offset);\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`);\n        }\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number');\n        }\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid offset metadata, expected string or null, received ${metadata}`);\n        }\n        const topicCommits = payload[topic] || [];\n        topicCommits.push({\n          partition,\n          offset: commitOffset,\n          metadata\n        });\n        return {\n          ...payload,\n          [topic]: topicCommits\n        };\n      }, {});\n      if (!consumerGroup) {\n        throw new KafkaJSNonRetriableError('Consumer group was not initialized, consumer#run must be called first');\n      }\n      const topics = Object.keys(commitsByTopic);\n      return runner.commitOffsets({\n        topics: topics.map(topic => {\n          return {\n            topic,\n            partitions: commitsByTopic[topic]\n          };\n        })\n      });\n    });\n    return function commitOffsets() {\n      return _ref8.apply(this, arguments);\n    };\n  }();\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({\n    topic,\n    partition,\n    offset\n  }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);\n    }\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(`Invalid partition, expected a number received ${partition}`);\n    }\n    let seekOffset;\n    try {\n      seekOffset = Long.fromValue(offset);\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`);\n    }\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number');\n    }\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError('Consumer group was not initialized, consumer#run must be called first');\n    }\n    consumerGroup.seek({\n      topic,\n      partition,\n      offset: seekOffset.toString()\n    });\n  };\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = /*#__PURE__*/function () {\n    var _ref9 = _asyncToGenerator(function* () {\n      const coordinator = yield cluster.findGroupCoordinator({\n        groupId\n      });\n      const retrier = createRetry(retry);\n      return retrier( /*#__PURE__*/_asyncToGenerator(function* () {\n        const {\n          groups\n        } = yield coordinator.describeGroups({\n          groupIds: [groupId]\n        });\n        return groups.find(group => group.groupId === groupId);\n      }));\n    });\n    return function describeGroup() {\n      return _ref9.apply(this, arguments);\n    };\n  }();\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(`Invalid topic ${topicPartition && topicPartition.topic || topicPartition}`);\n      } else if (typeof topicPartition.partitions !== 'undefined' && (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))) {\n        throw new KafkaJSNonRetriableError(`Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`);\n      }\n    }\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError('Consumer group was not initialized, consumer#run must be called first');\n    }\n    consumerGroup.pause(topicPartitions);\n  };\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return [];\n    }\n    return consumerGroup.paused();\n  };\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(`Invalid topic ${topicPartition && topicPartition.topic || topicPartition}`);\n      } else if (typeof topicPartition.partitions !== 'undefined' && (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))) {\n        throw new KafkaJSNonRetriableError(`Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`);\n      }\n    }\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError('Consumer group was not initialized, consumer#run must be called first');\n    }\n    consumerGroup.resume(topicPartitions);\n  };\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger;\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger\n  };\n};","map":{"version":3,"names":["Long","require","createRetry","initialRetryTime","ConsumerGroup","Runner","events","wrap","wrapEvent","unwrap","unwrapEvent","InstrumentationEventEmitter","KafkaJSNonRetriableError","roundRobin","EARLIEST_OFFSET","LATEST_OFFSET","ISOLATION_LEVEL","sharedPromiseTo","keys","values","Object","CONNECT","DISCONNECT","STOP","CRASH","eventNames","eventKeys","map","key","join","specialOffsets","fromValue","toString","module","exports","cluster","groupId","retry","logger","rootLogger","partitionAssigners","sessionTimeout","rebalanceTimeout","heartbeatInterval","maxBytesPerPartition","minBytes","maxBytes","maxWaitTimeInMs","isolationLevel","READ_COMMITTED","rackId","instrumentationEmitter","rootInstrumentationEmitter","metadataMaxAge","namespace","assigners","createAssigner","topics","runner","consumerGroup","restartTimeout","connect","emit","disconnect","stop","debug","e","error","message","stack","clearTimeout","info","subscribe","topic","subscriptionTopics","fromBeginning","Array","isArray","subscriptions","subscription","RegExp","hasRegexSubscriptions","some","metadata","undefined","topicsToSubscribe","isRegExp","topicRegExp","matchedTopics","topicMetadata","topicName","filter","test","push","t","addMultipleTargetTopics","run","autoCommit","autoCommitInterval","autoCommitThreshold","eachBatchAutoResolve","partitionsConsumedConcurrently","concurrency","eachBatch","eachMessage","warn","start","onCrash","topicConfigurations","name","retryCount","removeBroker","host","port","getOriginalCause","cause","isErrorRetriable","retriable","shouldRestart","restartOnFailure","catch","restart","retryTime","setTimeout","on","eventName","listener","includes","addListener","event","type","Promise","resolve","commitOffsets","topicPartitions","commitsByTopic","reduce","payload","partition","offset","isNaN","commitOffset","_","lessThan","topicCommits","partitions","seek","seekOffset","describeGroup","coordinator","findGroupCoordinator","retrier","groups","describeGroups","groupIds","find","group","pause","topicPartition","paused","resume","getLogger"],"sources":["C:/Users/ingev/Documents/Desarrollo/Luna/DronController/AD_UI/node_modules/kafkajs/src/consumer/index.js"],"sourcesContent":["const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n"],"mappings":";AAAA,MAAMA,IAAI,GAAGC,OAAO,CAAC,eAAe,CAAC;AACrC,MAAMC,WAAW,GAAGD,OAAO,CAAC,UAAU,CAAC;AACvC,MAAM;EAAEE;AAAiB,CAAC,GAAGF,OAAO,CAAC,mBAAmB,CAAC;AACzD,MAAMG,aAAa,GAAGH,OAAO,CAAC,iBAAiB,CAAC;AAChD,MAAMI,MAAM,GAAGJ,OAAO,CAAC,UAAU,CAAC;AAClC,MAAM;EAAEK,MAAM;EAAEC,IAAI,EAAEC,SAAS;EAAEC,MAAM,EAAEC;AAAY,CAAC,GAAGT,OAAO,CAAC,yBAAyB,CAAC;AAC3F,MAAMU,2BAA2B,GAAGV,OAAO,CAAC,4BAA4B,CAAC;AACzE,MAAM;EAAEW;AAAyB,CAAC,GAAGX,OAAO,CAAC,WAAW,CAAC;AACzD,MAAM;EAAEY;AAAW,CAAC,GAAGZ,OAAO,CAAC,aAAa,CAAC;AAC7C,MAAM;EAAEa,eAAe;EAAEC;AAAc,CAAC,GAAGd,OAAO,CAAC,cAAc,CAAC;AAClE,MAAMe,eAAe,GAAGf,OAAO,CAAC,4BAA4B,CAAC;AAC7D,MAAMgB,eAAe,GAAGhB,OAAO,CAAC,0BAA0B,CAAC;AAE3D,MAAM;EAAEiB,IAAI;EAAEC;AAAO,CAAC,GAAGC,MAAM;AAC/B,MAAM;EAAEC,OAAO;EAAEC,UAAU;EAAEC,IAAI;EAAEC;AAAM,CAAC,GAAGlB,MAAM;AAEnD,MAAMmB,UAAU,GAAGN,MAAM,CAACb,MAAM,CAAC;AACjC,MAAMoB,SAAS,GAAGR,IAAI,CAACZ,MAAM,CAAC,CAC3BqB,GAAG,CAACC,GAAG,IAAK,mBAAkBA,GAAI,EAAC,CAAC,CACpCC,IAAI,CAAC,IAAI,CAAC;AAEb,MAAMC,cAAc,GAAG,CACrB9B,IAAI,CAAC+B,SAAS,CAACjB,eAAe,CAAC,CAACkB,QAAQ,EAAE,EAC1ChC,IAAI,CAAC+B,SAAS,CAAChB,aAAa,CAAC,CAACiB,QAAQ,EAAE,CACzC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACAC,MAAM,CAACC,OAAO,GAAG,CAAC;EAChBC,OAAO;EACPC,OAAO;EACPC,KAAK;EACLC,MAAM,EAAEC,UAAU;EAClBC,kBAAkB,GAAG,CAAC3B,UAAU,CAAC;EACjC4B,cAAc,GAAG,KAAK;EACtBC,gBAAgB,GAAG,KAAK;EACxBC,iBAAiB,GAAG,IAAI;EACxBC,oBAAoB,GAAG,OAAO;EAAE;EAChCC,QAAQ,GAAG,CAAC;EACZC,QAAQ,GAAG,QAAQ;EAAE;EACrBC,eAAe,GAAG,IAAI;EACtBC,cAAc,GAAGhC,eAAe,CAACiC,cAAc;EAC/CC,MAAM,GAAG,EAAE;EACXC,sBAAsB,EAAEC,0BAA0B;EAClDC;AACF,CAAC,KAAK;EACJ,IAAI,CAACjB,OAAO,EAAE;IACZ,MAAM,IAAIxB,wBAAwB,CAAC,8CAA8C,CAAC;EACpF;EAEA,MAAM0B,MAAM,GAAGC,UAAU,CAACe,SAAS,CAAC,UAAU,CAAC;EAC/C,MAAMH,sBAAsB,GAAGC,0BAA0B,IAAI,IAAIzC,2BAA2B,EAAE;EAC9F,MAAM4C,SAAS,GAAGf,kBAAkB,CAACb,GAAG,CAAC6B,cAAc,IACrDA,cAAc,CAAC;IAAEpB,OAAO;IAAEE,MAAM;IAAEH;EAAQ,CAAC,CAAC,CAC7C;;EAED;EACA,MAAMsB,MAAM,GAAG,CAAC,CAAC;EACjB,IAAIC,MAAM,GAAG,IAAI;EACjB;EACA,IAAIC,aAAa,GAAG,IAAI;EACxB,IAAIC,cAAc,GAAG,IAAI;EAEzB,IAAIjB,iBAAiB,IAAIF,cAAc,EAAE;IACvC,MAAM,IAAI7B,wBAAwB,CAC/B,+BAA8B+B,iBAAkB,wCAAuCF,cAAe,+FAA8F,CACtM;EACH;;EAEA;EACA,MAAMoB,OAAO;IAAA,6BAAG,aAAY;MAC1B,MAAM1B,OAAO,CAAC0B,OAAO,EAAE;MACvBV,sBAAsB,CAACW,IAAI,CAACzC,OAAO,CAAC;IACtC,CAAC;IAAA,gBAHKwC,OAAO;MAAA;IAAA;EAAA,GAGZ;;EAED;EACA,MAAME,UAAU;IAAA,8BAAG,aAAY;MAC7B,IAAI;QACF,MAAMC,IAAI,EAAE;QACZ1B,MAAM,CAAC2B,KAAK,CAAC,qCAAqC,EAAE;UAAE7B;QAAQ,CAAC,CAAC;QAChE,MAAMD,OAAO,CAAC4B,UAAU,EAAE;QAC1BZ,sBAAsB,CAACW,IAAI,CAACxC,UAAU,CAAC;MACzC,CAAC,CAAC,OAAO4C,CAAC,EAAE;QACV5B,MAAM,CAAC6B,KAAK,CAAE,iDAAgDD,CAAC,CAACE,OAAQ,EAAC,EAAE;UACzEC,KAAK,EAAEH,CAAC,CAACG,KAAK;UACdjC;QACF,CAAC,CAAC;QACF,MAAM8B,CAAC;MACT;IACF,CAAC;IAAA,gBAbKH,UAAU;MAAA;IAAA;EAAA,GAaf;;EAED;EACA,MAAMC,IAAI,GAAG/C,eAAe,iCAAC,aAAY;IACvC,IAAI;MACF,IAAIyC,MAAM,EAAE;QACV,MAAMA,MAAM,CAACM,IAAI,EAAE;QACnBN,MAAM,GAAG,IAAI;QACbC,aAAa,GAAG,IAAI;QACpBR,sBAAsB,CAACW,IAAI,CAACvC,IAAI,CAAC;MACnC;MAEA+C,YAAY,CAACV,cAAc,CAAC;MAC5BtB,MAAM,CAACiC,IAAI,CAAC,SAAS,EAAE;QAAEnC;MAAQ,CAAC,CAAC;IACrC,CAAC,CAAC,OAAO8B,CAAC,EAAE;MACV5B,MAAM,CAAC6B,KAAK,CAAE,4CAA2CD,CAAC,CAACE,OAAQ,EAAC,EAAE;QACpEC,KAAK,EAAEH,CAAC,CAACG,KAAK;QACdjC;MACF,CAAC,CAAC;MAEF,MAAM8B,CAAC;IACT;EACF,CAAC,EAAC;;EAEF;EACA,MAAMM,SAAS;IAAA,8BAAG,WAAO;MAAEC,KAAK;MAAEhB,MAAM,EAAEiB,kBAAkB;MAAEC,aAAa,GAAG;IAAM,CAAC,EAAK;MACxF,IAAIhB,aAAa,EAAE;QACjB,MAAM,IAAI/C,wBAAwB,CAAC,qDAAqD,CAAC;MAC3F;MAEA,IAAI,CAAC6D,KAAK,IAAI,CAACC,kBAAkB,EAAE;QACjC,MAAM,IAAI9D,wBAAwB,CAAC,oCAAoC,CAAC;MAC1E;MAEA,IAAI8D,kBAAkB,IAAI,IAAI,IAAI,CAACE,KAAK,CAACC,OAAO,CAACH,kBAAkB,CAAC,EAAE;QACpE,MAAM,IAAI9D,wBAAwB,CAAC,oCAAoC,CAAC;MAC1E;MAEA,MAAMkE,aAAa,GAAGJ,kBAAkB,IAAI,CAACD,KAAK,CAAC;MAEnD,KAAK,MAAMM,YAAY,IAAID,aAAa,EAAE;QACxC,IAAI,OAAOC,YAAY,KAAK,QAAQ,IAAI,EAAEA,YAAY,YAAYC,MAAM,CAAC,EAAE;UACzE,MAAM,IAAIpE,wBAAwB,CAC/B,iBAAgBmE,YAAa,KAAI,OAAOA,YAAa,kDAAiD,CACxG;QACH;MACF;MAEA,MAAME,qBAAqB,GAAGH,aAAa,CAACI,IAAI,CAACH,YAAY,IAAIA,YAAY,YAAYC,MAAM,CAAC;MAChG,MAAMG,QAAQ,GAAGF,qBAAqB,SAAS9C,OAAO,CAACgD,QAAQ,EAAE,GAAGC,SAAS;MAE7E,MAAMC,iBAAiB,GAAG,EAAE;MAC5B,KAAK,MAAMN,YAAY,IAAID,aAAa,EAAE;QACxC,MAAMQ,QAAQ,GAAGP,YAAY,YAAYC,MAAM;QAC/C,IAAIM,QAAQ,EAAE;UACZ,MAAMC,WAAW,GAAGR,YAAY;UAChC,MAAMS,aAAa,GAAGL,QAAQ,CAACM,aAAa,CACzC9D,GAAG,CAAC,CAAC;YAAE8C,KAAK,EAAEiB;UAAU,CAAC,KAAKA,SAAS,CAAC,CACxCC,MAAM,CAACD,SAAS,IAAIH,WAAW,CAACK,IAAI,CAACF,SAAS,CAAC,CAAC;UAEnDpD,MAAM,CAAC2B,KAAK,CAAC,8BAA8B,EAAE;YAC3C7B,OAAO;YACPmD,WAAW,EAAEA,WAAW,CAACvD,QAAQ,EAAE;YACnCwD;UACF,CAAC,CAAC;UAEFH,iBAAiB,CAACQ,IAAI,CAAC,GAAGL,aAAa,CAAC;QAC1C,CAAC,MAAM;UACLH,iBAAiB,CAACQ,IAAI,CAACd,YAAY,CAAC;QACtC;MACF;MAEA,KAAK,MAAMe,CAAC,IAAIT,iBAAiB,EAAE;QACjC5B,MAAM,CAACqC,CAAC,CAAC,GAAG;UAAEnB;QAAc,CAAC;MAC/B;MAEA,MAAMxC,OAAO,CAAC4D,uBAAuB,CAACV,iBAAiB,CAAC;IAC1D,CAAC;IAAA,gBApDKb,SAAS;MAAA;IAAA;EAAA,GAoDd;;EAED;EACA,MAAMwB,GAAG;IAAA,8BAAG,WAAO;MACjBC,UAAU,GAAG,IAAI;MACjBC,kBAAkB,GAAG,IAAI;MACzBC,mBAAmB,GAAG,IAAI;MAC1BC,oBAAoB,GAAG,IAAI;MAC3BC,8BAA8B,EAAEC,WAAW,GAAG,CAAC;MAC/CC,SAAS,GAAG,IAAI;MAChBC,WAAW,GAAG;IAChB,CAAC,GAAG,CAAC,CAAC,EAAK;MACT,IAAI7C,aAAa,EAAE;QACjBrB,MAAM,CAACmE,IAAI,CAAC,8DAA8D,EAAE;UAAErE;QAAQ,CAAC,CAAC;QACxF;MACF;MAEA,MAAMsE,KAAK;QAAA,8BAAG,WAAMC,OAAO,EAAI;UAC7BrE,MAAM,CAACiC,IAAI,CAAC,UAAU,EAAE;YAAEnC;UAAQ,CAAC,CAAC;UAEpCuB,aAAa,GAAG,IAAIvD,aAAa,CAAC;YAChCkC,MAAM,EAAEC,UAAU;YAClBkB,MAAM,EAAEvC,IAAI,CAACuC,MAAM,CAAC;YACpBmD,mBAAmB,EAAEnD,MAAM;YAC3BpB,KAAK;YACLF,OAAO;YACPC,OAAO;YACPmB,SAAS;YACTd,cAAc;YACdC,gBAAgB;YAChBE,oBAAoB;YACpBC,QAAQ;YACRC,QAAQ;YACRC,eAAe;YACfI,sBAAsB;YACtBH,cAAc;YACdE,MAAM;YACNG,cAAc;YACd4C,UAAU;YACVC,kBAAkB;YAClBC;UACF,CAAC,CAAC;UAEFzC,MAAM,GAAG,IAAIrD,MAAM,CAAC;YAClBiC,MAAM,EAAEC,UAAU;YAClBoB,aAAa;YACbR,sBAAsB;YACtBR,iBAAiB;YACjBN,KAAK;YACL4D,UAAU;YACVG,oBAAoB;YACpBG,SAAS;YACTC,WAAW;YACXG,OAAO;YACPL;UACF,CAAC,CAAC;UAEF,MAAM5C,MAAM,CAACgD,KAAK,EAAE;QACtB,CAAC;QAAA,gBAzCKA,KAAK;UAAA;QAAA;MAAA,GAyCV;MAED,MAAMC,OAAO;QAAA,8BAAG,WAAMzC,CAAC,EAAI;UACzB5B,MAAM,CAAC6B,KAAK,CAAE,UAASD,CAAC,CAAC2C,IAAK,KAAI3C,CAAC,CAACE,OAAQ,EAAC,EAAE;YAC7ChC,OAAO;YACP0E,UAAU,EAAE5C,CAAC,CAAC4C,UAAU;YACxBzC,KAAK,EAAEH,CAAC,CAACG;UACX,CAAC,CAAC;UAEF,IAAIH,CAAC,CAAC2C,IAAI,KAAK,8BAA8B,EAAE;YAC7C1E,OAAO,CAAC4E,YAAY,CAAC;cAAEC,IAAI,EAAE9C,CAAC,CAAC8C,IAAI;cAAEC,IAAI,EAAE/C,CAAC,CAAC+C;YAAK,CAAC,CAAC;UACtD;UAEA,MAAMlD,UAAU,EAAE;UAElB,MAAMmD,gBAAgB,GAAG/C,KAAK,IAAI;YAChC,IAAIA,KAAK,CAACgD,KAAK,EAAE;cACf,OAAOD,gBAAgB,CAAC/C,KAAK,CAACgD,KAAK,CAAC;YACtC;YAEA,OAAOhD,KAAK;UACd,CAAC;UAED,MAAMiD,gBAAgB,GACpBlD,CAAC,CAAC2C,IAAI,KAAK,gCAAgC,IAAIK,gBAAgB,CAAChD,CAAC,CAAC,CAACmD,SAAS,KAAK,IAAI;UACvF,MAAMC,aAAa,GACjBF,gBAAgB,KACf,CAAC/E,KAAK,IACL,CAACA,KAAK,CAACkF,gBAAgB,WAChBlF,KAAK,CAACkF,gBAAgB,CAACrD,CAAC,CAAC,CAACsD,KAAK,CAACrD,KAAK,IAAI;YAC9C7B,MAAM,CAAC6B,KAAK,CACV,iGAAiG,EACjG;cACEA,KAAK,EAAEA,KAAK,CAACC,OAAO,IAAID,KAAK;cAC7BgD,KAAK,EAAEjD,CAAC,CAACE,OAAO,IAAIF,CAAC;cACrB9B;YACF,CAAC,CACF;YAED,OAAO,IAAI;UACb,CAAC,CAAC,CAAC,CAAC;UAERe,sBAAsB,CAACW,IAAI,CAACtC,KAAK,EAAE;YACjC2C,KAAK,EAAED,CAAC;YACR9B,OAAO;YACPqF,OAAO,EAAEH;UACX,CAAC,CAAC;UAEF,IAAIA,aAAa,EAAE;YACjB,MAAMI,SAAS,GAAGxD,CAAC,CAACwD,SAAS,IAAKrF,KAAK,IAAIA,KAAK,CAAClC,gBAAiB,IAAIA,gBAAgB;YACtFmC,MAAM,CAAC6B,KAAK,CAAE,8BAA6BuD,SAAU,IAAG,EAAE;cACxDZ,UAAU,EAAE5C,CAAC,CAAC4C,UAAU;cACxBY,SAAS;cACTtF;YACF,CAAC,CAAC;YAEFwB,cAAc,GAAG+D,UAAU,CAAC,MAAMjB,KAAK,CAACC,OAAO,CAAC,EAAEe,SAAS,CAAC;UAC9D;QACF,CAAC;QAAA,gBAxDKf,OAAO;UAAA;QAAA;MAAA,GAwDZ;MAED,MAAMD,KAAK,CAACC,OAAO,CAAC;IACtB,CAAC;IAAA,gBApHKX,GAAG;MAAA;IAAA;EAAA,GAoHR;;EAED;EACA,MAAM4B,EAAE,GAAG,CAACC,SAAS,EAAEC,QAAQ,KAAK;IAClC,IAAI,CAACrG,UAAU,CAACsG,QAAQ,CAACF,SAAS,CAAC,EAAE;MACnC,MAAM,IAAIjH,wBAAwB,CAAE,+BAA8Bc,SAAU,EAAC,CAAC;IAChF;IAEA,OAAOyB,sBAAsB,CAAC6E,WAAW,CAACtH,WAAW,CAACmH,SAAS,CAAC,EAAEI,KAAK,IAAI;MACzEA,KAAK,CAACC,IAAI,GAAG1H,SAAS,CAACyH,KAAK,CAACC,IAAI,CAAC;MAClCC,OAAO,CAACC,OAAO,CAACN,QAAQ,CAACG,KAAK,CAAC,CAAC,CAACT,KAAK,CAACtD,CAAC,IAAI;QAC1C5B,MAAM,CAAC6B,KAAK,CAAE,+BAA8BD,CAAC,CAACE,OAAQ,EAAC,EAAE;UACvDyD,SAAS;UACTxD,KAAK,EAAEH,CAAC,CAACG;QACX,CAAC,CAAC;MACJ,CAAC,CAAC;IACJ,CAAC,CAAC;EACJ,CAAC;;EAED;AACF;AACA;AACA;AACA;EACE,MAAMgE,aAAa;IAAA,8BAAG,WAAOC,eAAe,GAAG,EAAE,EAAK;MACpD,MAAMC,cAAc,GAAGD,eAAe,CAACE,MAAM,CAC3C,CAACC,OAAO,EAAE;QAAEhE,KAAK;QAAEiE,SAAS;QAAEC,MAAM;QAAExD,QAAQ,GAAG;MAAK,CAAC,KAAK;QAC1D,IAAI,CAACV,KAAK,EAAE;UACV,MAAM,IAAI7D,wBAAwB,CAAE,iBAAgB6D,KAAM,EAAC,CAAC;QAC9D;QAEA,IAAImE,KAAK,CAACF,SAAS,CAAC,EAAE;UACpB,MAAM,IAAI9H,wBAAwB,CAC/B,iDAAgD8H,SAAU,EAAC,CAC7D;QACH;QAEA,IAAIG,YAAY;QAChB,IAAI;UACFA,YAAY,GAAG7I,IAAI,CAAC+B,SAAS,CAAC4G,MAAM,CAAC;QACvC,CAAC,CAAC,OAAOG,CAAC,EAAE;UACV,MAAM,IAAIlI,wBAAwB,CAAE,4CAA2C+H,MAAO,EAAC,CAAC;QAC1F;QAEA,IAAIE,YAAY,CAACE,QAAQ,CAAC,CAAC,CAAC,EAAE;UAC5B,MAAM,IAAInI,wBAAwB,CAAC,sCAAsC,CAAC;QAC5E;QAEA,IAAIuE,QAAQ,KAAK,IAAI,IAAI,OAAOA,QAAQ,KAAK,QAAQ,EAAE;UACrD,MAAM,IAAIvE,wBAAwB,CAC/B,8DAA6DuE,QAAS,EAAC,CACzE;QACH;QAEA,MAAM6D,YAAY,GAAGP,OAAO,CAAChE,KAAK,CAAC,IAAI,EAAE;QAEzCuE,YAAY,CAACnD,IAAI,CAAC;UAAE6C,SAAS;UAAEC,MAAM,EAAEE,YAAY;UAAE1D;QAAS,CAAC,CAAC;QAEhE,OAAO;UAAE,GAAGsD,OAAO;UAAE,CAAChE,KAAK,GAAGuE;QAAa,CAAC;MAC9C,CAAC,EACD,CAAC,CAAC,CACH;MAED,IAAI,CAACrF,aAAa,EAAE;QAClB,MAAM,IAAI/C,wBAAwB,CAChC,uEAAuE,CACxE;MACH;MAEA,MAAM6C,MAAM,GAAGrC,MAAM,CAACF,IAAI,CAACqH,cAAc,CAAC;MAE1C,OAAO7E,MAAM,CAAC2E,aAAa,CAAC;QAC1B5E,MAAM,EAAEA,MAAM,CAAC9B,GAAG,CAAC8C,KAAK,IAAI;UAC1B,OAAO;YACLA,KAAK;YACLwE,UAAU,EAAEV,cAAc,CAAC9D,KAAK;UAClC,CAAC;QACH,CAAC;MACH,CAAC,CAAC;IACJ,CAAC;IAAA,gBAvDK4D,aAAa;MAAA;IAAA;EAAA,GAuDlB;;EAED;EACA,MAAMa,IAAI,GAAG,CAAC;IAAEzE,KAAK;IAAEiE,SAAS;IAAEC;EAAO,CAAC,KAAK;IAC7C,IAAI,CAAClE,KAAK,EAAE;MACV,MAAM,IAAI7D,wBAAwB,CAAE,iBAAgB6D,KAAM,EAAC,CAAC;IAC9D;IAEA,IAAImE,KAAK,CAACF,SAAS,CAAC,EAAE;MACpB,MAAM,IAAI9H,wBAAwB,CAC/B,iDAAgD8H,SAAU,EAAC,CAC7D;IACH;IAEA,IAAIS,UAAU;IACd,IAAI;MACFA,UAAU,GAAGnJ,IAAI,CAAC+B,SAAS,CAAC4G,MAAM,CAAC;IACrC,CAAC,CAAC,OAAOG,CAAC,EAAE;MACV,MAAM,IAAIlI,wBAAwB,CAAE,4CAA2C+H,MAAO,EAAC,CAAC;IAC1F;IAEA,IAAIQ,UAAU,CAACJ,QAAQ,CAAC,CAAC,CAAC,IAAI,CAACjH,cAAc,CAACiG,QAAQ,CAACoB,UAAU,CAACnH,QAAQ,EAAE,CAAC,EAAE;MAC7E,MAAM,IAAIpB,wBAAwB,CAAC,sCAAsC,CAAC;IAC5E;IAEA,IAAI,CAAC+C,aAAa,EAAE;MAClB,MAAM,IAAI/C,wBAAwB,CAChC,uEAAuE,CACxE;IACH;IAEA+C,aAAa,CAACuF,IAAI,CAAC;MAAEzE,KAAK;MAAEiE,SAAS;MAAEC,MAAM,EAAEQ,UAAU,CAACnH,QAAQ;IAAG,CAAC,CAAC;EACzE,CAAC;;EAED;EACA,MAAMoH,aAAa;IAAA,8BAAG,aAAY;MAChC,MAAMC,WAAW,SAASlH,OAAO,CAACmH,oBAAoB,CAAC;QAAElH;MAAQ,CAAC,CAAC;MACnE,MAAMmH,OAAO,GAAGrJ,WAAW,CAACmC,KAAK,CAAC;MAClC,OAAOkH,OAAO,iCAAC,aAAY;QACzB,MAAM;UAAEC;QAAO,CAAC,SAASH,WAAW,CAACI,cAAc,CAAC;UAAEC,QAAQ,EAAE,CAACtH,OAAO;QAAE,CAAC,CAAC;QAC5E,OAAOoH,MAAM,CAACG,IAAI,CAACC,KAAK,IAAIA,KAAK,CAACxH,OAAO,KAAKA,OAAO,CAAC;MACxD,CAAC,EAAC;IACJ,CAAC;IAAA,gBAPKgH,aAAa;MAAA;IAAA;EAAA,GAOlB;;EAED;AACF;AACA;AACA;AACA;EACE,MAAMS,KAAK,GAAG,CAACvB,eAAe,GAAG,EAAE,KAAK;IACtC,KAAK,MAAMwB,cAAc,IAAIxB,eAAe,EAAE;MAC5C,IAAI,CAACwB,cAAc,IAAI,CAACA,cAAc,CAACrF,KAAK,EAAE;QAC5C,MAAM,IAAI7D,wBAAwB,CAC/B,iBAAiBkJ,cAAc,IAAIA,cAAc,CAACrF,KAAK,IAAKqF,cAAe,EAAC,CAC9E;MACH,CAAC,MAAM,IACL,OAAOA,cAAc,CAACb,UAAU,KAAK,WAAW,KAC/C,CAACrE,KAAK,CAACC,OAAO,CAACiF,cAAc,CAACb,UAAU,CAAC,IAAIa,cAAc,CAACb,UAAU,CAAC/D,IAAI,CAAC0D,KAAK,CAAC,CAAC,EACpF;QACA,MAAM,IAAIhI,wBAAwB,CAC/B,8EAA6EkJ,cAAc,CAACb,UAAW,EAAC,CAC1G;MACH;IACF;IAEA,IAAI,CAACtF,aAAa,EAAE;MAClB,MAAM,IAAI/C,wBAAwB,CAChC,uEAAuE,CACxE;IACH;IAEA+C,aAAa,CAACkG,KAAK,CAACvB,eAAe,CAAC;EACtC,CAAC;;EAED;AACF;AACA;AACA;AACA;EACE,MAAMyB,MAAM,GAAG,MAAM;IACnB,IAAI,CAACpG,aAAa,EAAE;MAClB,OAAO,EAAE;IACX;IAEA,OAAOA,aAAa,CAACoG,MAAM,EAAE;EAC/B,CAAC;;EAED;AACF;AACA;AACA;AACA;EACE,MAAMC,MAAM,GAAG,CAAC1B,eAAe,GAAG,EAAE,KAAK;IACvC,KAAK,MAAMwB,cAAc,IAAIxB,eAAe,EAAE;MAC5C,IAAI,CAACwB,cAAc,IAAI,CAACA,cAAc,CAACrF,KAAK,EAAE;QAC5C,MAAM,IAAI7D,wBAAwB,CAC/B,iBAAiBkJ,cAAc,IAAIA,cAAc,CAACrF,KAAK,IAAKqF,cAAe,EAAC,CAC9E;MACH,CAAC,MAAM,IACL,OAAOA,cAAc,CAACb,UAAU,KAAK,WAAW,KAC/C,CAACrE,KAAK,CAACC,OAAO,CAACiF,cAAc,CAACb,UAAU,CAAC,IAAIa,cAAc,CAACb,UAAU,CAAC/D,IAAI,CAAC0D,KAAK,CAAC,CAAC,EACpF;QACA,MAAM,IAAIhI,wBAAwB,CAC/B,+EAA8EkJ,cAAc,CAACb,UAAW,EAAC,CAC3G;MACH;IACF;IAEA,IAAI,CAACtF,aAAa,EAAE;MAClB,MAAM,IAAI/C,wBAAwB,CAChC,uEAAuE,CACxE;IACH;IAEA+C,aAAa,CAACqG,MAAM,CAAC1B,eAAe,CAAC;EACvC,CAAC;;EAED;AACF;AACA;EACE,MAAM2B,SAAS,GAAG,MAAM3H,MAAM;EAE9B,OAAO;IACLuB,OAAO;IACPE,UAAU;IACVS,SAAS;IACTR,IAAI;IACJgC,GAAG;IACHqC,aAAa;IACba,IAAI;IACJE,aAAa;IACbS,KAAK;IACLE,MAAM;IACNC,MAAM;IACNpC,EAAE;IACFtH,MAAM;IACNgC,MAAM,EAAE2H;EACV,CAAC;AACH,CAAC"},"metadata":{},"sourceType":"script","externalDependencies":[]}